
5
Single chip photonic deep neural network with accelerated training
Saumil Bandyopadhyay,1, ∗ Alexander Sludds,1 Stefan Krastanov,1 Ryan Hamerly,1, 2 Nicholas
Harris,1 Darius Bunandar,1 Matthew Streshinsky,3 Michael Hochberg,4 and Dirk Englund1, †
1Research Laboratory of Electronics, MIT, Cambridge, MA 02139, USA
2NTT Research Inc., PHI Laboratories, 940 Stewart Drive, Sunnyvale, CA 94085, USA
3Nokia Corporation, New York, NY, 10016, USA
4Luminous Computing Inc., Mountain View, CA, 94041, USA
As deep neural networks (DNNs) revolutionize machine learning [1–6], energy consumption and through-
put are emerging as fundamental limitations of CMOS electronics. This has motivated a search for new
hardware architectures optimized for artificial intelligence, such as electronic systolic arrays [7], memristor
crossbar arrays [8], and optical accelerators. Optical systems can perform linear matrix operations at
exceptionally high rate and efficiency [9], motivating recent demonstrations of low latency linear algebra
[10–14] and optical energy consumption [15, 16] below a photon per multiply-accumulate operation.
However, demonstrating systems that co-integrate both linear and nonlinear processing units in a single
chip remains a central challenge. Here we introduce such a system in a scalable photonic integrated
circuit (PIC), enabled by several key advances: (i) high-bandwidth and low-power programmable nonlin-
ear optical function units (NOFUs); (ii) coherent matrix multiplication units (CMXUs); and (iii) in situ
training with optical acceleration. We experimentally demonstrate this fully-integrated coherent optical
neural network (FICONN) architecture for a 3-layer DNN comprising 12 NOFUs and three CMXUs oper-
ating in the telecom C-band. Using in situ training on a vowel classification task, the FICONN achieves
92.7% accuracy on a test set, which is identical to the accuracy obtained on a digital computer with
the same number of weights. This work lends experimental evidence to theoretical proposals for in situ
training, unlocking orders of magnitude improvements in the throughput of training data. Moreover,
the FICONN opens the path to inference at nanosecond latency and femtojoule per operation energy
efficiency.
State-of-the-art artificial intelligence algorithms, includ-
ing deep neural networks (DNNs), require vast amounts
of computation that is mostly dominated by linear alge-
bra. The sheer amount of computation required, together
with thermal cooling limits, has motivated the development
of new hardware in which energy efficiency is a key de-
sign parameter. This includes “output-stationary” optical
hardware [16, 17], which compute matrix-matrix products
by integrating optical signals over multiple time steps, and
“weight-stationary” architectures [10–15, 18] that compute
a single matrix-vector product per clock cycle.
Weight-stationary architectures in particular are well-
suited for applications that require processing data natively
in the optical domain and with ultra-low latency. For exam-
ple, self-driving cars require making split-second decisions
by processing and learning from sensor readings acquired
by LiDAR systems [19]; scientific research in astronomy
[20, 21] and particle physics [22] requires rapid analysis
of weak signals; and recently introduced “smart” optical
transceivers rely on machine learning to receive, process,
and route data at line rates exceeding hundreds of gigabits
per second [16, 23]. These applications present an opportu-
nity for real-time inference and training directly on optical
signals, preserving phase information and avoiding the need
for electrical-to-optical conversions.
Here we report the first demonstration of a fully-
integrated, coherent optical deep neural network that per-
∗ saumilb@mit.edu
† englund@mit.edu
forms inference and in situ training. This is made possible
by three key advances spanning devices to system hardware
to algorithms:
1. Coherent programmable optical nonlinearities: An
outstanding challenge for optical DNNs is realizing
fast, energy-efficient nonlinearities that can be inte-
grated into photonic circuits. Electro-optical activa-
tion functions have previously been realized in sili-
con photonics, but required off-chip, high-power elec-
tronic amplifiers [14, 24] to generate a sufficient non-
linear response. Here we design a nonlinear optical
function unit (NOFU) for DNNs that is fabricated in
a commercial foundry process, does not require an
amplifier, and implements a reconfigurable coherent
nonlinear operation on the optical field.
2. Coherent matrix multiplication units: Computing lin-
ear algebra coherently introduces the opportunity to
process optical signals, which contain information in
both amplitude and phase, directly within DNNs while
bypassing slow optical-to-electronic conversions. We
realize the DNN’s linear transformations with a coher-
ent matrix multiplication unit (CMXU) that computes
matrix-vector products through passive interference
in a Mach-Zehnder interferometer (MZI) mesh [25].
While previous demonstrations have used individual
MZI meshes to implement the linear layer of a DNN
[10, 26], here we integrate multiple CMXUs to realize
a coherent optical DNN on a single chip.
3. In situ training of coherent optical DNNs: The
efficient training of model parameters is a central
arXiv:2208.01623v1 [cs.ET] 2 Aug 2022
2
challenge in machine learning. A critical bottleneck
for model training is forward inference, as it requires
many evaluations of the model on a large training
set to optimize weight parameters. In situ training
on photonic hardware can take advantage of near-
instantaneous DNN inference, lowering the latency
and power consumption of model training. Moreover,
learning weights in real time can benefit applications
that natively process optical data, such as LiDAR
systems [19], optical transceivers [23], and federated
learning for edge devices [16, 27].
While prior work has shown training of individ-
ual linear layers of photonic neural networks in situ
[28, 29], finding efficient algorithms for training
all-photonic DNNs, which include on-chip optical
nonlinearities, remains a major obstacle. Here we
report, to the best of our knowledge, the first
demonstration of in situ training of a fully-integrated
photonic DNN. We demonstrate efficient, optically-
accelerated training of the hardware to perform a
vowel classification task with 92.7% accuracy, which
is the same as the accuracy obtained on a digital
computer. Our approach, which computes deriva-
tives directly on hardware, generalizes beyond the
system presented here to the many other photonic
architectures for DNNs being currently studied.
Our system, comprising 132 individually tunable model pa-
rameters on-chip, is the largest integrated photonic DNN
demonstrated to date in number of weights and computes
matrix operations and nonlinear activation functions coher-
ently on optical fields. We combined all subsystems for
a coherent optical DNN into a photonic integrated circuit
(PIC), fabricated in a commercial silicon photonics process,
that incorporates both programmable linear and nonlinear
transformations onto a single 6 × 5.7 mm2 chip.
ARCHITECTURE
Figure 1 shows how this PIC architecture allows us to
realize a fully integrated coherent optical neural network
(FICONN) through the following stages: (i) the transmitter
(TX) maps input vectors x(j) to an optical field vector a(1)
(j)
by splitting an input laser field into MZI modulators m =
1, 2, ..., 6, each of which encode one element of x(j) into
the amplitude Am and phase φm of the transverse electric
field component a(1)
(j),m = Ameiωt+φm ; (ii) the coherent ma-
trix multiplication unit (CMXU), consisting of a MZI mesh
[10, 26, 30], transforms a(1) → b(1) = U(1)a(1) through
passive optical interference; and (iii) the programmable non-
linear optical function unit (NOFU) applies the activation
function to yield the input to the next layer, a(2) = f (b(1)).
Following the input layer, the PIC directly transmits the
optically-encoded signal into a hidden layer, composed of
another CMXU and six NOFUs, that implements the trans-
formation a(3) = f (U(2)a(2)). The final layer U(3), imple-
mented with a third CMXU, maps a(3) to the output b(3).
Inference therefore proceeds entirely in the optical domain
without photodiode readout, amplifiers, or digitization be-
tween layers.
An integrated coherent receiver (ICR), shown in Figure
1(iv), reads out the amplitude and phase of the DNN output
by homodyning each element of the vector b(3) with a com-
mon local oscillator field ELO. The DNN output is read out
by transimpedance amplifiers that convert the photocurrent
vector iPD to a voltage vector VPD . VPD is digitized and
then normalized by the sum of voltages measured across
all channels ∑ VPD to yield a quasi-probability distribu-
tion Vnorm for a classification task. Each sample x(i) is
assigned the label corresponding to the highest probability,
i.e. argmax(Vnorm).
EXPERIMENT
We implemented the FICONN architecture in a commer-
cial silicon photonic foundry process incorporating low-loss
edge couplers and waveguides, compact phase shifters, high-
speed waveguide-integrated germanium photodiodes, and
efficient carrier-based microring modulators.
Figure 2a shows the PIC, fabricated in a silicon-on-
insulator (SOI) process, which monolithically integrates all
FICONN subsystems. Demonstrating our system, which re-
quired control of 169 active devices and stable optical cou-
pling, necessitated developing a custom photonic package
for lab testing. This package, shown in Figure 2b, interfaces
the active devices on chip to driver electronics through 236
wirebonds to a printed circuit board. Input light is coupled
into the circuit through a single channel of a polarization-
maintaining fiber array glued to the chip facet with index-
matching epoxy. No light is coupled out of the PIC, as all
readout is done on chip with the ICR.
We measured an end-to-end loss for our system of 10 dB,
including 2.5 dB fiber-to-chip coupling loss. As the depth of
our system is 91 layers of optical components from input to
readout, the end-to-end loss implies a per-component inser-
tion loss of less than 0.1 dB, enabling single-shot inference
across all DNN layers without optical re-amplification.
The key subsystems of the PIC are depicted in Figures
2c−f. In Figure 2c, we show the transmitter for encoding
input vectors into the FICONN. The light coupled into the
chip is first split with an MZI into a local oscillator (LO)
path, which is directed to the ICR, and a signal path, which
is fanned out to six channels. Each channel of the trans-
mitter comprises an MZI, which programs the amplitude of
one element of a(1)
(j) , and a phase shifter on the output that
encodes the phase. As the inset shows, a typical channel re-
alizes more than 40 dB of extinction, enabling programming
of input vectors with more than 13 bits of precision.
Figure 2d shows the coherent matrix multiplication unit
(CMXU), which computes linear transformations in the
DNN. The CMXU is comprised of a MZI mesh [30] of 15 de-
vices, connected in the Clements configuration [31], which
implements an arbitrary 6 × 6 unitary operation U(1) on the
3
FIG. 1: Architecture of the fully-integrated coherent optical neural network (FICONN). Inference is conducted entirely in the opti-
cal domain, without readout or amplification between layers. Light is fiber coupled into a single input on the chip and fanned out
to the six channels of the transmitter (i). Each channel encodes the amplitude and phase of one element of the input x(j) into the
optical field a(1)
(j) with a Mach-Zehnder modulator and an external phase shifter. The coherent matrix multiplication unit (ii), con-
sisting of a Mach-Zehnder interferometer mesh, implements the linear transformation b(n)
(j) = U(n)a(n)
(j) . Programmable nonlinear op-
tical function units (iii) realize activation functions a(n+1)
(j) = f (b(n)
(j) ) by tapping off part of the signal to a photodiode, which drives
a cavity off-resonance by injecting carriers into the waveguide. An integrated coherent receiver (iv) reads out the DNN output by
homodyning the output field with a local oscillator. Transimpedance amplifiers convert the output photocurrents to voltages, which
are digitized and normalized to produce a quasi-probability distribution for a classification task. During in situ training, the model
parameters Θ are recurrently optimized to minimize the categorical cross-entropy over the training set xtrain
(1),(2),...(N).
optical fields a(1). As det(U(1)) = 1, the CMXU conserves
optical power in the system with the exception of compo-
nent insertion losses. Unitary weighting, which redistributes
light between optical modes but does not attenuate it, min-
imizes optical losses and enables single-shot DNN inference
without re-amplification or readout between layers. Train-
ing unitary layers also avoids the vanishing gradient prob-
lem, improving optimization of deep and recurrent neural
networks [32].
We benchmarked the matrix accuracy of the CMXU by
programming 500 random 6 × 6 unitary matrices sam-
pled from the Haar measure into the device and measur-
ing the fidelity F = Tr[U†
programmedUmeasured]/6. In the his-
togram in Figure 2d, we show the measured fidelity ob-
tained with a “direct” programming, where we algorith-
mically decompose the phase shifter settings as outlined
in [31], and using a modified programming that corrects
for hardware errors, losses, and thermal crosstalk [33–35].
While a direct programming only achieves a matrix fidelity of
〈F 〉 = 0.900 ± 0.031, correcting for hardware non-idealities
improves this value to 〈F 〉 = 0.987 ± 0.007 for the CMXU.
To the best of our knowledge, this is the highest reported
fidelity for a programmable photonic matrix processor.
Figure 2e shows the integrated coherent receiver (ICR),
which measures the amplitude and phase of the output sig-
nal b(3) of the DNN. Each channel, as shown in Figure 2f,
interferes the signal field with the LO using a 50-50 multi-
mode interferometer (MMI) and measures the outputs with
a pair of balanced detectors. A phase shifter is used to
select the quadrature being read out.
The programmable nonlinear optical function unit
(NOFU) is shown in Figure 3. To realize a programmable
4
FIG. 2: a) Microscope image of the fabricated PIC. Key subsystems of the circuit are highlighted in the same color as the archi-
tecture depicted in Figure 1. The signal path through the PIC is indicated in white, while the local oscillator path is outlined in
blue. b) Photonic packaging of the PIC for lab testing. Insets show side and top-down views of the packaged PIC. c) The fabri-
cated transmitter splits off light coupled into the PIC to a local oscillator and fans out the remainder to six input channels. The
inset shows the measured optical response of a typical channel. d) The coherent matrix multiplication unit is implemented with a
Mach-Zehnder interferometer mesh. Each MZI comprises two directional couplers (DCs), an internal phase shifter θ1 between the
two splitters, and an external phase shifter θ2 on one output mode. The histogram shows the measured fidelity of 500 arbitrary
unitary matrices implemented on a single layer using a “direct” approach (orange) and an approach that takes into account hard-
ware errors and thermal crosstalk (blue). e) The integrated coherent receiver (ICR). f) One channel of the ICR. Signal and LO are
interfered on a 50-50 MMI and measured using balanced detectors.
coherent optical activation function, we developed the reso-
nant electro-optical nonlinearity shown schematically in Fig-
ure 1iii). This device directs a fraction β of the incident
optical power |b|2 into a photodiode by programming the
phase shift θ in an MZI. The photodiode is electrically con-
nected to a pn-doped resonant microring modulator, and
the resultant photocurrent (or photovoltage) detunes the
resonance by either injecting (or depleting) carriers from
the waveguide. The remainder of the incident signal field
passes into the microring resonator; the nonlinear modula-
tion of the electric field b by the cavity, which is dependent
on the incident optical power |b|2, results in a coherent non-
5
FIG. 3: a) The fabricated NOFU. A programmable MZI determines the fraction of light tapped off to the photodiode, and a
waveguide delay line synchronizes the optical and electrical pulses. A pn-doped microring resonator modulates the incident field.
b) Circuit diagram of resonant EO nonlinearity. The photocurrent Ip directly drives a pn-doped resonant modulator. No amplifier
stage is required between the two and the devices are directly connected on chip. By adjusting the bias voltage VB , the nonlinear-
ity can be operated in forward or reverse bias. c) Left: Detuning of the cavity resonance at various incident optical powers when
operated in carrier injection mode (VB > 0). Right: Cavity detuning in carrier depletion mode (VB < 0). Our system realizes
close to a linewidth detuning without the use of any amplifier, improving energy consumption and latency of the nonlinearity. A full
linewidth detuning can be realized by further engineering the cavity finesse. d) Activation functions measured on chip. Arbitrary
function shapes can be realized by adjusting the cavity detuning ∆λ and fraction of light β tapped off to the photodiode.
linear optical function for DNNs. Setting the detuning of
the cavity and the fraction of optical power tapped off to
the photodiode determines the implemented function.
Figure 3a shows the fabricated device, where the photodi-
ode output is directly connected on chip to the modulator.
An integrated heater aligns the microring resonance to the
programmed detuning, and an optical delay line placed be-
tween the tunable coupler and modulator synchronizes the
optical and electrical pulses.
The electrical circuit for the NOFU is shown in Figure
3b. Incident light generates a reverse current in the photo-
diode; depending on the bias voltage VB , this either injects
carriers into the modulator or generates a photovoltage that
depletes the modulator of carriers. Figure 3c shows the de-
vice response in injection (left) and depletion modes (right).
In injection mode, optical power modulates both the loss
and phase of the resonator, producing a strong nonlinear
response to the incident field b. In depletion mode, we ob-
serve nearly a linewidth detuning when the incident light is
switched on vs. off, which is induced by the voltage pro-
duced by the photodiode.
The NOFU is designed to implement programmable non-
linear activation functions at high speeds with ultra-low en-
ergy consumption. This required separately optimizing the
cavity parameters, which determine the microring response
time, and closely integrating the photodiode and modulator
together on the PIC to minimize total device capacitance,
and therefore the RC time delay. In injection mode, we
found that 75 μA photocurrent was sufficient to detune
the resonator by a linewidth. As each NOFU performs the
equivalent of two multiplications in digital electronics, over
a carrier lifetime of ∼1 ns this corresponds to an energy
consumption of 30 fJ per nonlinear operation (NLOP).
Compared to prior approaches [14, 24], the NOFU directly
drives the modulator through the photodiode and eliminates
the amplifier stage between them. This greatly improves the
latency and energy efficiency of the device, as high speed
transimpedance amplifiers consume hundreds of milliwatts
of power [36]. For our device, incorporating such an ampli-
fier would have increased the power consumption by two or-
ders of magnitude to about 3 pJ/NLOP. Our design, which
eliminates intermediate amplifier circuitry and is therefore
“receiverless” [37], is not only more energy-efficient, but
also eliminates the latency introduced by the amplifier.
In Figure 3d, we show several of the activation functions
measured on chip. The programmability of the device en-
6
FIG. 4: a) A multivariate cost function L(Θ) can be minimized by computing the directional derivative of the function along a
random direction (black). This directs the optimization along the component of the gradient (red) parallel to the search direction.
Over multiple iterations, the steps taken along random directions average to follow the direction of steepest descent to the mini-
mum. b) In situ training procedure. At every iteration, the directional derivative of the cost function L(Θ) is computed in hard-
ware along a randomly chosen direction ∆ in the search space. ∆ is chosen from a Bernoulli distribution to be ±δ. The weights Θ
are then updated by the measured derivative following a learning rate η chosen as a hyperparameter of the optimization. c) In situ
training of a photonic DNN for vowel classification. We obtain 92.7% accuracy on a test set, which is the same as the performance
(92.7%) obtained on a digital model with the same number of weights. Despite not having direct access to gradients, our approach
produces a training curve similar to those produced by standard gradient descent algorithms.
ables a wide range of nonlinear optical functions to be re-
alized. By tuning the fraction of power tapped off to the
photodiode and the relative detuning of the cavity, we can
not only program the form of the nonlinear function, but
also train it during model optimization.
IN SITU TRAINING
The accuracy of the inference output depends on the
model parameters Θ of the FICONN system, comprising a
total of Nmodel = NlayerN2
neuron + 2Nneuron(Nlayer − 1) = 132
real-valued phase shifter settings controlled with 16 bits of
precision. These parameters can either be determined of-
fline by training on a digital computer [10, 11, 16], using a
digital model of the hardware [38], or by training the hard-
ware parameters in situ.
Training in situ takes advantage of low latency inference
on optical hardware, reducing the time and energy required
for model optimization. Previous work on in situ train-
ing has focused on developing optical implementations of
“backpropagation,” which is the standard for training elec-
tronic DNNs [28, 39]. However, these approaches train only
the linear layers of a photonic system and require evaluat-
ing gradients of activation functions on a digital system,
thereby limiting the optical acceleration obtained by com-
puting a multi-layer DNN in a single shot. Alternatively,
genetic algorithms have been used to optimize weights on
chip [29], but they are challenging to scale to large model
sizes and require many generations to converge.
We trained the model parameters of the FICONN in situ,
including those of the activation functions, by evaluating the
derivatives of those parameters directly on the hardware. To
the best of our knowledge, this is the first demonstration of
in situ training of a photonic DNN. Our approach, which
is based on prior work on in situ optimization of analog
VLSI neural networks [40, 41], is robust to noise, performs
gradient descent on average, and is guaranteed to converge
to a local minimum. Moreover, it is not limited to our
specific system, but can be generalized to any hardware
architecture for photonic DNNs.
A direct approach to computing the gradient on hard-
7
ware would be to perturb the model parameters Θ =
[Θ1, Θ2, ..., ΘN ] one weight at a time and repeatedly batch
the training set through the system [10]. This procedure
produces a forward difference estimate of the loss gradient
∇L(Θ) with respect to all weights. Moreover, since the
derivatives are evaluated directly on chip, this procedure ex-
tends to other hardware parameters, such as the detuning
and fraction of power tapped off in the NOFU. The draw-
back to this approach is that for N parameters, it requires
batching the training set through the hardware 2N times.
Our approach varies all model parameters Θ simultane-
ously. Figure 4b sketches the optimization procedure. In-
stead of perturbing the parameters one weight at a time,
during training the system perturbs all parameters towards
a random direction ∆ in search space, i.e. Θ → Θ + ∆ =
Θ + [δ1, δ2, ..., δN ]. At each iteration the system then com-
putes the directional derivative:
∇∆L(Θ) = L(Θ + ∆) − L(Θ − ∆)
2||∆|| (1)
As in standard gradient descent, the weights Θ are then
updated to Θ → Θ − η∇∆L(Θ)∆, where η is a learning
rate chosen as a hyperparameter of the system.
Compared to the forward difference approach outlined
earlier, our approach requires batching the training set
through the hardware only twice per iteration. Moreover, we
obtain true estimates of the cost function L and the deriva-
tive ∇∆L(W), ensuring that component errors or errors in
calibration do not affect the accuracy of training. Unlike
other derivative-free optimization methods, our approach
will always track the direction of steepest descent, as errors
in the gradient direction average out to zero over multiple
epochs [40, 41] (see Supplementary Information [SI]).
We implemented in situ training of Θ, which includes
weights and nonlinear function parameters, for a standard
vowel classification task (dataset available at [42]). At each
epoch, we batched a training set of 540 samples into the
system and implemented the optimization loop described in
Figure 4b with a learning rate η = 0.002. We reserved part
of the data (N = 294) to evaluate the trained model on
inputs it had not seen before.
The top plot of Figure 4c shows the classification accu-
racy of both datasets during training. Our system achieves
over 96% accuracy on the training set, and over 92% ac-
curacy on the test set, as shown in the confusion matrices
at the bottom. When training a digital system, we found
it also obtained the same accuracy on the test set. Each
epoch batches the training set only three times through the
system; two times to evaluate the derivative ∇∆L(Θ) and
once more to evaluate L(Θ) at the current parameter set Θ.
We observed that the system quickly trained to an accuracy
exceeding 80%, and then slowly asymptoted to a training
accuracy of 96%. This behavior resembles the optimization
trajectories of other first-order methods for training DNNs in
electronics, such as stochastic gradient descent. Moreover,
our system successfully trains using only 16-bit accuracies
for the weights. Lower precision weights reduce memory
requirements for training; however, digital systems are chal-
lenging to train with fewer than 32 bits due to numerical er-
rors in gradients accumulating during backpropagation [43].
DISCUSSION
An important DNN metric is the latency τlatency of in-
ference, i.e. the time delay between input of a vector
and the DNN output. For the FICONN, τlatency is dom-
inated by the optical propagation delay, which we esti-
mate from the PIC subsystems (as described in the SI) as
3τCMXU + 2τNOFU + τTX to U1 + τU3 to RX + τU-turn ≈ 435 ps.
Each inference requires NOPS ≈ 2MN2 matrix operations
(see SI), where M is the number of layers and N the number
of modes. Dividing the FICONN’s energy consumed during
τlatency by NOPS upper-bounds the energy-per-operation as
EOP ≈ τlatency
2
[
PPS + PNOFU
N + PTX + PICR
MN
]
, (2)
where Pj denotes the power dissipation of subsystem j. In
the SI, we estimate upper bounds to the on-chip energy con-
sumption of 9.8 pJ/OP and a throughput of NOPS/τlatency ≈
0.53 tera-operations per second (TOPS) per inference.
N Phase shifter EOP Etotal,est τlatency TOPS
6 Thermal
(this work)
9.8 pJ/OP 11.7 pJ/OP 435 ps 0.53
6 Undercut
thermal [44]
35 fJ/OP 546 fJ/OP 140 ps 12
6 MEMS [45, 46] 1.6 fJ/OP 513 fJ/OP 140 ps 12
64 MEMS [45, 46] 0.84 fJ/OP 54 fJ/OP 1.4 ns 1240
128 MEMS [45, 46] 0.79 fJ/OP 27 fJ/OP 2.7 ns 4940
256 MEMS [45, 46] 0.77 fJ/OP 14 fJ/OP 5.4 ns 19700
TABLE I: Performance metrics for a three-layer FICONN with
N neurons. We list the on-chip energy consumption EOP, as
well as an estimate of the total power dissipation Etotal,est in-
cluding optimized driver electronics. The predicted metrics
assume inference on large batches of vectors with resonant
modulators at 50 GHz [47]. For latency, we assume a device
length of 500 μm and an optimized layout, while our reported
latency uses the actual waveguide layout fabricated on the PIC.
The FICONN’s power consumption is dominated by the
thermal phase shifters, which require ∼25 mW of electrical
power to produce a π phase shift. Table I lists the key pa-
rameters of our proof-of-concept FICONN (top row), along
with estimates for alternative published phase shifter tech-
nologies for varying N and M = 3. These estimates suggest
that low-power quasi-static phase shifters in combination
with high-speed modulators [47] could push total energy
consumption to ∼ 10 fJ/OP for large systems, while main-
taining ns latencies and throughputs of thousands of TOPS.
In comparison, systolic arrays such as the tensor processing
unit (TPU) require at minimum N +1 clock cycles for a sin-
gle N × N matrix-vector multiplication. A three-layer DNN
with N = 256 neurons would require ∼1 μs to compute at
